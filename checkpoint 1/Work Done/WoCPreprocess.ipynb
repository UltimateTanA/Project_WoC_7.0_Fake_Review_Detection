{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "STEP 1:\n",
        "\n",
        "DATA CLEANING"
      ],
      "metadata": {
        "id": "vOrcN5g5Vjjc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJL1V8MYp1oB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('fakeReviewData.csv')  # Adjust for file type, e.g., .csv or .xlsx"
      ],
      "metadata": {
        "id": "3bzPK8iLuG_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FL15JYCuZiv",
        "outputId": "9f2a290c-932a-431e-c878-74872a4a57b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \n",
            "0  Love this!  Well made, sturdy, and very comfor...  \n",
            "1  love it, a great upgrade from the original.  I...  \n",
            "2  This pillow saved my back. I love the look and...  \n",
            "3  Missing information on how to use it, but it i...  \n",
            "4  Very nice set. Good quality. We have had the s...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#here we can see what the data looks like, the shape, the number of null values, column names\n",
        "\n",
        "print(df.shape)\n",
        "print(df.columns)\n",
        "print(df.isnull().sum())\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LskEi7Ygu1i8",
        "outputId": "6c7f7dc2-63f7-43ce-da45-1113f3330162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40432, 4)\n",
            "Index(['category', 'rating', 'label', 'text_'], dtype='object')\n",
            "category    0\n",
            "rating      0\n",
            "label       0\n",
            "text_       0\n",
            "dtype: int64\n",
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \n",
            "0  Love this!  Well made, sturdy, and very comfor...  \n",
            "1  love it, a great upgrade from the original.  I...  \n",
            "2  This pillow saved my back. I love the look and...  \n",
            "3  Missing information on how to use it, but it i...  \n",
            "4  Very nice set. Good quality. We have had the s...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['category'].unique())\n",
        "print(df['label'].unique())\n",
        "print(df['rating'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb80KhYuxc-R",
        "outputId": "4096a106-dec8-4b9e-fd0e-6e93d39684bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Home_and_Kitchen_5' 'Sports_and_Outdoors_5' 'Electronics_5'\n",
            " 'Movies_and_TV_5' 'Tools_and_Home_Improvement_5' 'Pet_Supplies_5'\n",
            " 'Kindle_Store_5' 'Books_5' 'Toys_and_Games_5'\n",
            " 'Clothing_Shoes_and_Jewelry_5']\n",
            "['CG' 'OR']\n",
            "[5. 1. 3. 2. 4.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#in this dataset, we have every datapoint as a review at an online marketplace.\n",
        "#here, we can afford to drop duplicates since a duplicate review doesn't hold value for us.\n",
        "\n",
        "df = df.drop_duplicates()\n"
      ],
      "metadata": {
        "id": "8oo4K8EOx5r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 2:\n",
        "\n",
        "TEXT NORMALIZATION"
      ],
      "metadata": {
        "id": "z78kWUPbVprT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#here we'll work on the review text itself.\n",
        "#we want to perform the following:\n",
        "#1. remove numbers, punctuations, special charactesr\n",
        "#2. convert all upper case characters to lower case\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_text(value):\n",
        "    value = value.lower()\n",
        "    value = re.sub(r'[^a-z\\s]', '', value)\n",
        "    return value\n",
        "\n",
        "df['text_'] = df['text_'].apply(clean_text)"
      ],
      "metadata": {
        "id": "-GXZEvINykrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"text_\"].head(15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmnThNL4ypD8",
        "outputId": "581aca8a-83e5-4e56-e89a-efd1bd31d033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0     love this  well made sturdy and very comfortab...\n",
            "1     love it a great upgrade from the original  ive...\n",
            "2     this pillow saved my back i love the look and ...\n",
            "3     missing information on how to use it but it is...\n",
            "4     very nice set good quality we have had the set...\n",
            "5           i wanted different flavors but they are not\n",
            "6     they are the perfect touch for me and the only...\n",
            "7     these done fit well and look great  i love the...\n",
            "8     great big numbers  easy to read the only thing...\n",
            "9     my son loves this comforter and it is very wel...\n",
            "10    as advertised th one ive had the only problem ...\n",
            "11    very handy for one of my kids and the tools ar...\n",
            "12    did someone say oriental for   it is a great p...\n",
            "13    these are so flimsy they are not the quality y...\n",
            "14    makes may tea with out stirring the only probl...\n",
            "Name: text_, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 3:\n",
        "\n",
        "TOKENIZATION\n"
      ],
      "metadata": {
        "id": "34RGUXWVVvEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "\n",
        "# Apply tokenization\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "# Tokenize with progress bar\n",
        "df['tokens'] = df['text_'].progress_apply(lambda x: [token.text for token in nlp(x)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "4viQ6pc0Ueg4",
        "outputId": "3143d686-f7d8-461f-a1c7-4ee043bf63c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-64bef5054df5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Reapply tokenization to ensure lists of words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Verify the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-64bef5054df5>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Reapply tokenization to ensure lists of words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Verify the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-v9arvhyeohv",
        "outputId": "c44264e6-9d1c-4e31-ae0b-d28ed2b411ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \n",
            "0  love this  well made sturdy and very comfortab...  \n",
            "1  love it a great upgrade from the original  ive...  \n",
            "2  this pillow saved my back i love the look and ...  \n",
            "3  missing information on how to use it but it is...  \n",
            "4  very nice set good quality we have had the set...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the above code snippet took 15 mins to run. i'll now be downloading this dataset to prevent having to run it every time."
      ],
      "metadata": {
        "id": "qhjYy6IReYwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export to a CSV file\n",
        "df.to_csv('fakeReviewDataTokenized.csv', index=False)  # Set index=False to avoid saving the index column\n"
      ],
      "metadata": {
        "id": "wnCFCGcTeYIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"fakeReviewDataTokenized.csv\")"
      ],
      "metadata": {
        "id": "yAIjFIKMe9qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 4:\n",
        "\n",
        "STOPWORD REMOVAL\n"
      ],
      "metadata": {
        "id": "gVfrH24UfHth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "# Convert stringified lists to actual lists\n",
        "df['tokens'] = df['tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "# Verify the output\n",
        "print(df['tokens'].iloc[0])  # Check the first row's tokens\n",
        "print(type(df['tokens'].iloc[0]))  # Confirm type is a list\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQg3KQvek-IH",
        "outputId": "053bca5e-07f2-4bd7-89bb-448760567ab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['love', 'this', ' ', 'well', 'made', 'sturdy', 'and', 'very', 'comfortable', ' ', 'i', 'love', 'itvery', 'pretty']\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords\n",
        "df['tokens_cleaned'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])\n",
        "\n",
        "# Verify cleaned tokens\n",
        "print(df['tokens_cleaned'].iloc[0])  # Check the first row's cleaned tokens\n",
        "print(type(df['tokens_cleaned'].iloc[0]))  # Confirm type is a list\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNq6OuDglaYm",
        "outputId": "897419f1-e6e6-413f-97ae-809f05969fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['love', ' ', 'well', 'made', 'sturdy', 'comfortable', ' ', 'love', 'itvery', 'pretty']\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2Ufv70flauv",
        "outputId": "e2e7df9a-1dd4-493c-93e8-da79f65fd1a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \\\n",
            "0  love this  well made sturdy and very comfortab...   \n",
            "1  love it a great upgrade from the original  ive...   \n",
            "2  this pillow saved my back i love the look and ...   \n",
            "3  missing information on how to use it but it is...   \n",
            "4  very nice set good quality we have had the set...   \n",
            "\n",
            "                                              tokens  \\\n",
            "0  [love, this,  , well, made, sturdy, and, very,...   \n",
            "1  [love, it, a, great, upgrade, from, the, origi...   \n",
            "2  [this, pillow, saved, my, back, i, love, the, ...   \n",
            "3  [missing, information, on, how, to, use, it, b...   \n",
            "4  [very, nice, set, good, quality, we, have, had...   \n",
            "\n",
            "                                      tokens_cleaned  \n",
            "0  [love,  , well, made, sturdy, comfortable,  , ...  \n",
            "1  [love, great, upgrade, original,  , mine, coup...  \n",
            "2    [pillow, saved, back, love, look, feel, pillow]  \n",
            "3  [missing, information, use, great, product, pr...  \n",
            "4       [nice, set, good, quality, set, two, months]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out spaces and empty strings\n",
        "df['tokens_cleaned'] = df['tokens_cleaned'].apply(lambda tokens: [word for word in tokens if word.strip()])\n",
        "print(df[['tokens', 'tokens_cleaned']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nStFRu_lnMY",
        "outputId": "2c71318e-79bf-49e6-929f-eb29d54c8955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              tokens  \\\n",
            "0  [love, this,  , well, made, sturdy, and, very,...   \n",
            "1  [love, it, a, great, upgrade, from, the, origi...   \n",
            "2  [this, pillow, saved, my, back, i, love, the, ...   \n",
            "3  [missing, information, on, how, to, use, it, b...   \n",
            "4  [very, nice, set, good, quality, we, have, had...   \n",
            "\n",
            "                                      tokens_cleaned  \n",
            "0  [love, well, made, sturdy, comfortable, love, ...  \n",
            "1  [love, great, upgrade, original, mine, couple,...  \n",
            "2    [pillow, saved, back, love, look, feel, pillow]  \n",
            "3  [missing, information, use, great, product, pr...  \n",
            "4       [nice, set, good, quality, set, two, months]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 5.1:\n",
        "\n",
        "STEMMING\n"
      ],
      "metadata": {
        "id": "AyVxUbhwo7o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Apply stemming to the cleaned tokens\n",
        "df['tokens_stemmed'] = df['tokens_cleaned'].apply(lambda tokens: [stemmer.stem(word) for word in tokens])\n",
        "\n",
        "# Verify the results\n",
        "print(df[['tokens_cleaned', 'tokens_stemmed']].head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjKt6_4Yl6aO",
        "outputId": "967f149d-59ac-4f41-a8d9-e536b6378cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      tokens_cleaned  \\\n",
            "0  [love, well, made, sturdy, comfortable, love, ...   \n",
            "1  [love, great, upgrade, original, mine, couple,...   \n",
            "2    [pillow, saved, back, love, look, feel, pillow]   \n",
            "3  [missing, information, use, great, product, pr...   \n",
            "4       [nice, set, good, quality, set, two, months]   \n",
            "\n",
            "                                      tokens_stemmed  \n",
            "0  [love, well, made, sturdi, comfort, love, itve...  \n",
            "1   [love, great, upgrad, origin, mine, coupl, year]  \n",
            "2     [pillow, save, back, love, look, feel, pillow]  \n",
            "3         [miss, inform, use, great, product, price]  \n",
            "4        [nice, set, good, qualiti, set, two, month]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 5.2:\n",
        "\n",
        "LEMMATIZATION"
      ],
      "metadata": {
        "id": "qAdpFYWYpBtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Apply lemmatization\n",
        "df['tokens_lemmatized'] = df['tokens_cleaned'].apply(lambda tokens: [token.lemma_ for token in nlp(\" \".join(tokens))])\n",
        "\n",
        "# Verify results\n",
        "print(df[['tokens_stemmed', 'tokens_lemmatized']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7neckz1ko8YN",
        "outputId": "1c8e3349-3df2-47d1-a58c-da181278ded3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      tokens_cleaned  \\\n",
            "0  [love, well, made, sturdy, comfortable, love, ...   \n",
            "1  [love, great, upgrade, original, mine, couple,...   \n",
            "2    [pillow, saved, back, love, look, feel, pillow]   \n",
            "3  [missing, information, use, great, product, pr...   \n",
            "4       [nice, set, good, quality, set, two, months]   \n",
            "\n",
            "                                   tokens_lemmatized  \n",
            "0  [love, well, make, sturdy, comfortable, love, ...  \n",
            "1  [love, great, upgrade, original, mine, couple,...  \n",
            "2     [pillow, save, back, love, look, feel, pillow]  \n",
            "3    [miss, information, use, great, product, price]  \n",
            "4        [nice, set, good, quality, set, two, month]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['tokens_stemmed', 'tokens_lemmatized']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkfHxyfesbrf",
        "outputId": "a8104292-abe4-4aa1-ed9b-70f83eb5e637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      tokens_stemmed  \\\n",
            "0  [love, well, made, sturdi, comfort, love, itve...   \n",
            "1   [love, great, upgrad, origin, mine, coupl, year]   \n",
            "2     [pillow, save, back, love, look, feel, pillow]   \n",
            "3         [miss, inform, use, great, product, price]   \n",
            "4        [nice, set, good, qualiti, set, two, month]   \n",
            "\n",
            "                                   tokens_lemmatized  \n",
            "0  [love, well, make, sturdy, comfortable, love, ...  \n",
            "1  [love, great, upgrade, original, mine, couple,...  \n",
            "2     [pillow, save, back, love, look, feel, pillow]  \n",
            "3    [miss, information, use, great, product, price]  \n",
            "4        [nice, set, good, quality, set, two, month]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('fakeReviewDataLemmatized.csv', index=False)  # Set index=False to avoid saving the index column\n"
      ],
      "metadata": {
        "id": "jahk0bPtq45y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"fakeReviewDataLemmatized.csv\")"
      ],
      "metadata": {
        "id": "DgsevL_rsA2s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6:\n",
        "\n",
        "VECTORIZATION"
      ],
      "metadata": {
        "id": "MXjnqkrQnF3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train a Word2Vec model on the tokens\n",
        "word2vec_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Example: Get the embedding for a word\n",
        "print(word2vec_model.wv['love'])  # Replace 'love' with any word in your corpus\n",
        "\n",
        "# Calculate sentence embeddings (average of word vectors)\n",
        "df['sentence_embedding'] = df['tokens'].apply(\n",
        "    lambda tokens: sum(word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv) / len(tokens)\n",
        ")\n",
        "\n",
        "# Verify the embeddings\n",
        "print(df['sentence_embedding'].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgwUaDfunIGP",
        "outputId": "bd97b5da-d914-4a34-f267-b0cda89393fb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.9379928  -2.2582974   2.0822835  -0.14541158  1.766413   -0.5553301\n",
            " -2.3091087  -0.46849772 -0.35740802  1.4148506   0.11011387  0.10354994\n",
            " -0.2593716  -0.55676     0.25424042  0.7575684  -0.7773697  -0.7532801\n",
            "  1.7922465  -0.82747966  1.5918834   1.9882152   1.4189023   1.930057\n",
            " -1.2273922  -2.2690227   2.6369216  -1.5885016  -1.7315971  -1.9388423\n",
            "  0.8566928  -1.6960273   1.8993409   1.753758    2.9653609  -1.7323052\n",
            " -0.72560734 -1.4164388  -1.8512759  -0.48776168  3.9412568   3.4486923\n",
            "  0.14409673 -0.6739967  -0.83567905 -0.41162592 -1.3563464  -0.3039591\n",
            " -0.08753868 -3.4055538   1.5397207  -2.2270803   0.81422985 -0.8166975\n",
            " -1.7349569  -0.39080548 -0.02773979  1.0049574  -1.8453878  -3.6597562\n",
            " -0.46632826 -1.3162625  -0.71918184  0.06219748  1.6589711   1.1551015\n",
            " -0.76251054 -0.75425065  1.8021358  -0.02483344  1.7671657   1.2352922\n",
            "  0.06451735  0.16145071 -0.1312562  -1.5268155   1.9326936   1.6353112\n",
            "  0.54105705  1.9310467  -2.007759    1.7014151  -1.4828423  -0.7028136\n",
            " -1.0483136   3.5891807   3.8269036   2.6026149  -1.9486111   4.005833\n",
            "  1.5129987  -1.6956663   0.41541192 -0.44178945 -1.5646015  -1.3586617\n",
            " -2.3224266  -0.826964    1.2799075  -1.1474607 ]\n",
            "0    [-0.78779465, -1.0975616, 0.5348595, 0.2781761...\n",
            "1    [-0.6225792, -0.13455379, -0.22366804, 0.64031...\n",
            "2    [-1.1129545, 0.18486145, 0.8021425, -0.1185586...\n",
            "3    [-0.43876353, -0.15118945, 0.016494717, -0.220...\n",
            "4    [0.0933322, 0.29551753, -0.09992492, 0.4984856...\n",
            "Name: sentence_embedding, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Word2Vec model\n",
        "word2vec_model.save(\"word2vec_model.model\")\n"
      ],
      "metadata": {
        "id": "PtnGXiUzrQeG"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}