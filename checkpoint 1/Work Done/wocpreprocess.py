# -*- coding: utf-8 -*-
"""WoCPreprocess.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LIre6fQu71hLrnX8gc9weno2BG69XzQv

STEP 1:

DATA CLEANING
"""

import pandas as pd

df = pd.read_csv('fakeReviewData.csv')  # Adjust for file type, e.g., .csv or .xlsx

print(df.head())

#here we can see what the data looks like, the shape, the number of null values, column names

print(df.shape)
print(df.columns)
print(df.isnull().sum())
print(df.head())

print(df['category'].unique())
print(df['label'].unique())
print(df['rating'].unique())

#in this dataset, we have every datapoint as a review at an online marketplace.
#here, we can afford to drop duplicates since a duplicate review doesn't hold value for us.

df = df.drop_duplicates()

"""STEP 2:

TEXT NORMALIZATION
"""

#here we'll work on the review text itself.
#we want to perform the following:
#1. remove numbers, punctuations, special charactesr
#2. convert all upper case characters to lower case


import re

def clean_text(value):
    value = value.lower()
    value = re.sub(r'[^a-z\s]', '', value)
    return value

df['text_'] = df['text_'].apply(clean_text)

print(df["text_"].head(15))

"""STEP 3:

TOKENIZATION

"""

import spacy

# Load spaCy model

# Apply tokenization
from tqdm import tqdm
tqdm.pandas()

# Tokenize with progress bar
df['tokens'] = df['text_'].progress_apply(lambda x: [token.text for token in nlp(x)])

print(df.head())

"""the above code snippet took 15 mins to run. i'll now be downloading this dataset to prevent having to run it every time."""

# Export to a CSV file
df.to_csv('fakeReviewDataTokenized.csv', index=False)  # Set index=False to avoid saving the index column

df = pd.read_csv("fakeReviewDataTokenized.csv")

"""STEP 4:

STOPWORD REMOVAL

"""

import ast

# Convert stringified lists to actual lists
df['tokens'] = df['tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)

# Verify the output
print(df['tokens'].iloc[0])  # Check the first row's tokens
print(type(df['tokens'].iloc[0]))  # Confirm type is a list

from nltk.corpus import stopwords
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

# Remove stopwords
df['tokens_cleaned'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])

# Verify cleaned tokens
print(df['tokens_cleaned'].iloc[0])  # Check the first row's cleaned tokens
print(type(df['tokens_cleaned'].iloc[0]))  # Confirm type is a list

print(df.head())

# Filter out spaces and empty strings
df['tokens_cleaned'] = df['tokens_cleaned'].apply(lambda tokens: [word for word in tokens if word.strip()])
print(df[['tokens', 'tokens_cleaned']].head())

"""STEP 5.1:

STEMMING

"""

from nltk.stem import PorterStemmer

# Initialize the stemmer
stemmer = PorterStemmer()

# Apply stemming to the cleaned tokens
df['tokens_stemmed'] = df['tokens_cleaned'].apply(lambda tokens: [stemmer.stem(word) for word in tokens])

# Verify the results
print(df[['tokens_cleaned', 'tokens_stemmed']].head())

"""STEP 5.2:

LEMMATIZATION
"""

import spacy

# Load the spaCy model
nlp = spacy.load("en_core_web_sm")

# Apply lemmatization
df['tokens_lemmatized'] = df['tokens_cleaned'].apply(lambda tokens: [token.lemma_ for token in nlp(" ".join(tokens))])

# Verify results
print(df[['tokens_stemmed', 'tokens_lemmatized']].head())

print(df[['tokens_stemmed', 'tokens_lemmatized']].head())

df.to_csv('fakeReviewDataLemmatized.csv', index=False)  # Set index=False to avoid saving the index column

import pandas as pd
df = pd.read_csv("fakeReviewDataLemmatized.csv")

"""6:

VECTORIZATION
"""

from gensim.models import Word2Vec

# Train a Word2Vec model on the tokens
word2vec_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=1, workers=4)

# Example: Get the embedding for a word
print(word2vec_model.wv['love'])  # Replace 'love' with any word in your corpus

# Calculate sentence embeddings (average of word vectors)
df['sentence_embedding'] = df['tokens'].apply(
    lambda tokens: sum(word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv) / len(tokens)
)

# Verify the embeddings
print(df['sentence_embedding'].head())

# Save the Word2Vec model
word2vec_model.save("word2vec_model.model")