{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "STEP 1:\n",
        "\n",
        "DATA CLEANING"
      ],
      "metadata": {
        "id": "vOrcN5g5Vjjc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sJL1V8MYp1oB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('fakeReviewData.csv')  # Adjust for file type, e.g., .csv or .xlsx"
      ],
      "metadata": {
        "id": "3bzPK8iLuG_t"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FL15JYCuZiv",
        "outputId": "6e2e960f-897c-4c6d-de63-339410a5d94a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \n",
            "0  Love this!  Well made, sturdy, and very comfor...  \n",
            "1  love it, a great upgrade from the original.  I...  \n",
            "2  This pillow saved my back. I love the look and...  \n",
            "3  Missing information on how to use it, but it i...  \n",
            "4  Very nice set. Good quality. We have had the s...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#here we can see what the data looks like, the shape, the number of null values, column names\n",
        "\n",
        "print(df.shape)\n",
        "print(df.columns)\n",
        "print(df.isnull().sum())\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LskEi7Ygu1i8",
        "outputId": "6e5933cd-cf3b-4276-cb75-3ee43e47806e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40432, 4)\n",
            "Index(['category', 'rating', 'label', 'text_'], dtype='object')\n",
            "category    0\n",
            "rating      0\n",
            "label       0\n",
            "text_       0\n",
            "dtype: int64\n",
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \n",
            "0  Love this!  Well made, sturdy, and very comfor...  \n",
            "1  love it, a great upgrade from the original.  I...  \n",
            "2  This pillow saved my back. I love the look and...  \n",
            "3  Missing information on how to use it, but it i...  \n",
            "4  Very nice set. Good quality. We have had the s...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['category'].unique())\n",
        "print(df['label'].unique())\n",
        "print(df['rating'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb80KhYuxc-R",
        "outputId": "f82d3a1a-4eef-4461-d1ca-8d4d6a6841cf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Home_and_Kitchen_5' 'Sports_and_Outdoors_5' 'Electronics_5'\n",
            " 'Movies_and_TV_5' 'Tools_and_Home_Improvement_5' 'Pet_Supplies_5'\n",
            " 'Kindle_Store_5' 'Books_5' 'Toys_and_Games_5'\n",
            " 'Clothing_Shoes_and_Jewelry_5']\n",
            "['CG' 'OR']\n",
            "[5. 1. 3. 2. 4.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#in this dataset, we have every datapoint as a review at an online marketplace.\n",
        "#here, we can afford to drop duplicates since a duplicate review doesn't hold value for us.\n",
        "\n",
        "df = df.drop_duplicates()\n"
      ],
      "metadata": {
        "id": "8oo4K8EOx5r6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 2:\n",
        "\n",
        "TEXT NORMALIZATION"
      ],
      "metadata": {
        "id": "z78kWUPbVprT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#here we'll work on the review text itself.\n",
        "#we want to perform the following:\n",
        "#1. remove numbers, punctuations, special charactesr\n",
        "#2. convert all upper case characters to lower case\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_text(value):\n",
        "    value = value.lower()\n",
        "    value = re.sub(r'[^a-z\\s]', '', value)\n",
        "    return value\n",
        "\n",
        "df['text_'] = df['text_'].apply(clean_text)"
      ],
      "metadata": {
        "id": "-GXZEvINykrT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"text_\"].head(15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmnThNL4ypD8",
        "outputId": "204951c6-2338-4dc0-9381-90522624f930"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0     love this  well made sturdy and very comfortab...\n",
            "1     love it a great upgrade from the original  ive...\n",
            "2     this pillow saved my back i love the look and ...\n",
            "3     missing information on how to use it but it is...\n",
            "4     very nice set good quality we have had the set...\n",
            "5           i wanted different flavors but they are not\n",
            "6     they are the perfect touch for me and the only...\n",
            "7     these done fit well and look great  i love the...\n",
            "8     great big numbers  easy to read the only thing...\n",
            "9     my son loves this comforter and it is very wel...\n",
            "10    as advertised th one ive had the only problem ...\n",
            "11    very handy for one of my kids and the tools ar...\n",
            "12    did someone say oriental for   it is a great p...\n",
            "13    these are so flimsy they are not the quality y...\n",
            "14    makes may tea with out stirring the only probl...\n",
            "Name: text_, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 3:\n",
        "\n",
        "TOKENIZATION\n"
      ],
      "metadata": {
        "id": "34RGUXWVVvEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Tokenize with progress bar\n",
        "df['tokens'] = df['text_'].progress_apply(lambda x: [token.text for token in nlp(x)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4viQ6pc0Ueg4",
        "outputId": "1934221b-5c70-4e74-b9e2-e56e81375e91"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40420/40420 [13:08<00:00, 51.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-v9arvhyeohv",
        "outputId": "570ad646-cbe1-4603-f71f-4072d9b303b3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \\\n",
            "0  love this  well made sturdy and very comfortab...   \n",
            "1  love it a great upgrade from the original  ive...   \n",
            "2  this pillow saved my back i love the look and ...   \n",
            "3  missing information on how to use it but it is...   \n",
            "4  very nice set good quality we have had the set...   \n",
            "\n",
            "                                              tokens  \n",
            "0  [love, this,  , well, made, sturdy, and, very,...  \n",
            "1  [love, it, a, great, upgrade, from, the, origi...  \n",
            "2  [this, pillow, saved, my, back, i, love, the, ...  \n",
            "3  [missing, information, on, how, to, use, it, b...  \n",
            "4  [very, nice, set, good, quality, we, have, had...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the above code snippet took 15 mins to run. i'll now be downloading this dataset to prevent having to run it every time."
      ],
      "metadata": {
        "id": "qhjYy6IReYwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export to a CSV file\n",
        "df.to_csv('fakeReviewDataTokenized.csv', index=False)  # Set index=False to avoid saving the index column\n"
      ],
      "metadata": {
        "id": "wnCFCGcTeYIa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"fakeReviewDataTokenized.csv\")"
      ],
      "metadata": {
        "id": "yAIjFIKMe9qK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 4:\n",
        "\n",
        "STOPWORD REMOVAL\n"
      ],
      "metadata": {
        "id": "gVfrH24UfHth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "# Convert stringified lists to actual lists\n",
        "df['tokens'] = df['tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "# Verify the output\n",
        "print(df['tokens'].iloc[0])  # Check the first row's tokens\n",
        "print(type(df['tokens'].iloc[0]))  # Confirm type is a list\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQg3KQvek-IH",
        "outputId": "21abaa85-ef70-41a1-cda7-7fed692ebe40"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['love', 'this', ' ', 'well', 'made', 'sturdy', 'and', 'very', 'comfortable', ' ', 'i', 'love', 'itvery', 'pretty']\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords\n",
        "df['tokens_cleaned'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])\n",
        "\n",
        "# Verify cleaned tokens\n",
        "print(df['tokens_cleaned'].iloc[0])  # Check the first row's cleaned tokens\n",
        "print(type(df['tokens_cleaned'].iloc[0]))  # Confirm type is a list\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNq6OuDglaYm",
        "outputId": "8390485b-308b-4ee9-df39-079161fdbb63"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['love', ' ', 'well', 'made', 'sturdy', 'comfortable', ' ', 'love', 'itvery', 'pretty']\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2Ufv70flauv",
        "outputId": "4eb76dd8-dfb7-4c18-c92c-0ba2cf350864"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \\\n",
            "0  love this  well made sturdy and very comfortab...   \n",
            "1  love it a great upgrade from the original  ive...   \n",
            "2  this pillow saved my back i love the look and ...   \n",
            "3  missing information on how to use it but it is...   \n",
            "4  very nice set good quality we have had the set...   \n",
            "\n",
            "                                              tokens  \\\n",
            "0  [love, this,  , well, made, sturdy, and, very,...   \n",
            "1  [love, it, a, great, upgrade, from, the, origi...   \n",
            "2  [this, pillow, saved, my, back, i, love, the, ...   \n",
            "3  [missing, information, on, how, to, use, it, b...   \n",
            "4  [very, nice, set, good, quality, we, have, had...   \n",
            "\n",
            "                                      tokens_cleaned  \n",
            "0  [love,  , well, made, sturdy, comfortable,  , ...  \n",
            "1  [love, great, upgrade, original,  , mine, coup...  \n",
            "2    [pillow, saved, back, love, look, feel, pillow]  \n",
            "3  [missing, information, use, great, product, pr...  \n",
            "4       [nice, set, good, quality, set, two, months]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out spaces and empty strings\n",
        "df['tokens_cleaned'] = df['tokens_cleaned'].apply(lambda tokens: [word for word in tokens if word.strip()])\n",
        "print(df[['tokens', 'tokens_cleaned']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nStFRu_lnMY",
        "outputId": "2a874329-333e-431a-ae71-110ebe6ef200"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              tokens  \\\n",
            "0  [love, this,  , well, made, sturdy, and, very,...   \n",
            "1  [love, it, a, great, upgrade, from, the, origi...   \n",
            "2  [this, pillow, saved, my, back, i, love, the, ...   \n",
            "3  [missing, information, on, how, to, use, it, b...   \n",
            "4  [very, nice, set, good, quality, we, have, had...   \n",
            "\n",
            "                                      tokens_cleaned  \n",
            "0  [love, well, made, sturdy, comfortable, love, ...  \n",
            "1  [love, great, upgrade, original, mine, couple,...  \n",
            "2    [pillow, saved, back, love, look, feel, pillow]  \n",
            "3  [missing, information, use, great, product, pr...  \n",
            "4       [nice, set, good, quality, set, two, months]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 5.1:\n",
        "\n",
        "STEMMING\n"
      ],
      "metadata": {
        "id": "AyVxUbhwo7o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Apply stemming to the cleaned tokens\n",
        "df['tokens_stemmed'] = df['tokens_cleaned'].apply(lambda tokens: [stemmer.stem(word) for word in tokens])\n",
        "\n",
        "# Verify the results\n",
        "print(df[['tokens_cleaned', 'tokens_stemmed']].head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjKt6_4Yl6aO",
        "outputId": "960451f4-8365-4c15-94cd-0f64b41bedf4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      tokens_cleaned  \\\n",
            "0  [love, well, made, sturdy, comfortable, love, ...   \n",
            "1  [love, great, upgrade, original, mine, couple,...   \n",
            "2    [pillow, saved, back, love, look, feel, pillow]   \n",
            "3  [missing, information, use, great, product, pr...   \n",
            "4       [nice, set, good, quality, set, two, months]   \n",
            "\n",
            "                                      tokens_stemmed  \n",
            "0  [love, well, made, sturdi, comfort, love, itve...  \n",
            "1   [love, great, upgrad, origin, mine, coupl, year]  \n",
            "2     [pillow, save, back, love, look, feel, pillow]  \n",
            "3         [miss, inform, use, great, product, price]  \n",
            "4        [nice, set, good, qualiti, set, two, month]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 5.2:\n",
        "\n",
        "LEMMATIZATION"
      ],
      "metadata": {
        "id": "qAdpFYWYpBtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Apply lemmatization\n",
        "df['tokens_lemmatized'] = df['tokens_cleaned'].apply(lambda tokens: [token.lemma_ for token in nlp(\" \".join(tokens))])\n",
        "\n",
        "# Verify results\n",
        "print(df[['tokens_stemmed', 'tokens_lemmatized']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7neckz1ko8YN",
        "outputId": "06670571-479c-48e6-e3ed-3daa3b87d699"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      tokens_stemmed  \\\n",
            "0  [love, well, made, sturdi, comfort, love, itve...   \n",
            "1   [love, great, upgrad, origin, mine, coupl, year]   \n",
            "2     [pillow, save, back, love, look, feel, pillow]   \n",
            "3         [miss, inform, use, great, product, price]   \n",
            "4        [nice, set, good, qualiti, set, two, month]   \n",
            "\n",
            "                                   tokens_lemmatized  \n",
            "0  [love, well, make, sturdy, comfortable, love, ...  \n",
            "1  [love, great, upgrade, original, mine, couple,...  \n",
            "2     [pillow, save, back, love, look, feel, pillow]  \n",
            "3    [miss, information, use, great, product, price]  \n",
            "4        [nice, set, good, quality, set, two, month]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['tokens_stemmed', 'tokens_lemmatized']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkfHxyfesbrf",
        "outputId": "3488cb98-5b54-4d1d-8caa-b3aa28e326b3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      tokens_stemmed  \\\n",
            "0  [love, well, made, sturdi, comfort, love, itve...   \n",
            "1   [love, great, upgrad, origin, mine, coupl, year]   \n",
            "2     [pillow, save, back, love, look, feel, pillow]   \n",
            "3         [miss, inform, use, great, product, price]   \n",
            "4        [nice, set, good, qualiti, set, two, month]   \n",
            "\n",
            "                                   tokens_lemmatized  \n",
            "0  [love, well, make, sturdy, comfortable, love, ...  \n",
            "1  [love, great, upgrade, original, mine, couple,...  \n",
            "2     [pillow, save, back, love, look, feel, pillow]  \n",
            "3    [miss, information, use, great, product, price]  \n",
            "4        [nice, set, good, quality, set, two, month]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('fakeReviewDataLemmatized.csv', index=False)  # Set index=False to avoid saving the index column\n"
      ],
      "metadata": {
        "id": "jahk0bPtq45y"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"fakeReviewDataLemmatized.csv\")"
      ],
      "metadata": {
        "id": "DgsevL_rsA2s"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6:\n",
        "\n",
        "VECTORIZATION"
      ],
      "metadata": {
        "id": "MXjnqkrQnF3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "# Convert stringified lists to actual lists\n",
        "df['tokens'] = df['tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "# Verify the output\n",
        "print(df['tokens'].iloc[0])  # Check the first row's tokens\n",
        "print(type(df['tokens'].iloc[0]))  # Confirm type is a list\n"
      ],
      "metadata": {
        "id": "DjOSjwEfs6dL",
        "outputId": "b0f0d680-000f-49bf-c4c0-c4ef588b144d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['love', 'this', ' ', 'well', 'made', 'sturdy', 'and', 'very', 'comfortable', ' ', 'i', 'love', 'itvery', 'pretty']\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train a Word2Vec model on the tokens\n",
        "word2vec_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Calculate sentence embeddings (average of word vectors)\n",
        "df['sentence_embedding'] = df['tokens'].apply(\n",
        "    lambda tokens: sum(word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv) / len(tokens)\n",
        ")\n",
        "\n",
        "# Verify the embeddings\n",
        "print(df['sentence_embedding'].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgwUaDfunIGP",
        "outputId": "ab527d1c-eb3e-47ee-a326-718062a8cd5c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    [-1.2240342, -0.72104365, 0.41656366, -0.40086...\n",
            "1    [-1.0465335, 0.033930317, 0.33292958, 0.469406...\n",
            "2    [-1.4938077, 0.27488175, 0.8935994, -0.6465594...\n",
            "3    [-0.9308116, -0.029670544, 0.5803284, -0.62010...\n",
            "4    [-0.61544967, 0.12781528, 0.5377493, 0.0713582...\n",
            "Name: sentence_embedding, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Word2Vec model\n",
        "word2vec_model.save(\"word2vec_model.model\")\n"
      ],
      "metadata": {
        "id": "PtnGXiUzrQeG"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}